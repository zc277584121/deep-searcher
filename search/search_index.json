{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udd0d DeepSearcher","text":""},{"location":"#overview","title":"\u2728 Overview","text":"<p>DeepSearcher combines cutting-edge LLMs (OpenAI o1, o3-mini, DeepSeek, Grok 3, Claude 3.7 Sonnet, Llama 4, QwQ, etc.) and Vector Databases (Milvus, Zilliz Cloud etc.) to perform search, evaluation, and reasoning based on private data, providing highly accurate answers and comprehensive reports.</p> <p>Perfect for: Enterprise knowledge management, intelligent Q&amp;A systems, and information retrieval scenarios.</p> <p></p>"},{"location":"#key-features","title":"\ud83d\ude80 Key Features","text":"Feature Description \ud83d\udd12 Private Data Search Maximizes utilization of enterprise internal data while ensuring data security. When necessary, integrates online content for more accurate answers. \ud83d\uddc4\ufe0f Vector Database Management Supports Milvus and other vector databases, allowing data partitioning for efficient retrieval. \ud83e\udde9 Flexible Embedding Options Compatible with multiple embedding models for optimal selection based on your needs. \ud83e\udd16 Multiple LLM Support Supports DeepSeek, OpenAI, and other large models for intelligent Q&amp;A and content generation. \ud83d\udcc4 Document Loader Supports local file loading, with web crawling capabilities under development."},{"location":"#demo","title":"\ud83c\udfac Demo","text":""},{"location":"future_plans/","title":"Future Plans","text":"<ul> <li>Enhance web crawling functionality</li> <li>Support more vector databases (e.g., FAISS...)</li> <li>Add support for additional large models</li> <li>Provide RESTful API interface (DONE)</li> </ul> <p>We welcome contributions! Star &amp; Fork the project and help us build a more powerful DeepSearcher! \ud83c\udfaf </p>"},{"location":"configuration/","title":"Configuration Overview","text":"<p>DeepSearcher provides flexible configuration options for all its components. You can customize the following aspects of the system:</p>"},{"location":"configuration/#components","title":"\ud83d\udccb Components","text":"Component Purpose Documentation LLM Large Language Models for query processing LLM Configuration Embedding Models Text embedding for vector retrieval Embedding Models Vector Database Storage and retrieval of vector embeddings Vector Database File Loader Loading and processing various file formats File Loader Web Crawler Gathering information from web sources Web Crawler"},{"location":"configuration/#configuration-method","title":"\ud83d\udd04 Configuration Method","text":"<p>DeepSearcher uses a consistent configuration approach for all components:</p> <pre><code>from deepsearcher.configuration import Configuration, init_config\n\n# Create configuration\nconfig = Configuration()\n\n# Set provider configurations\nconfig.set_provider_config(\"[component]\", \"[provider]\", {\"option\": \"value\"})\n\n# Initialize with configuration\ninit_config(config=config)\n</code></pre> <p>For detailed configuration options for each component, please visit the corresponding documentation pages linked in the table above.</p>"},{"location":"configuration/embedding/","title":"Embedding Model Configuration","text":"<p>DeepSearcher supports various embedding models to convert text into vector representations for semantic search.</p>"},{"location":"configuration/embedding/#basic-configuration","title":"\ud83d\udcdd Basic Configuration","text":"<pre><code>config.set_provider_config(\"embedding\", \"(EmbeddingModelName)\", \"(Arguments dict)\")\n</code></pre>"},{"location":"configuration/embedding/#available-embedding-providers","title":"\ud83d\udccb Available Embedding Providers","text":"Provider Description Key Features OpenAIEmbedding OpenAI's text embedding models High quality, production-ready MilvusEmbedding Built-in embedding models via Pymilvus Multiple model options VoyageEmbedding VoyageAI embedding models Specialized for search BedrockEmbedding Amazon Bedrock embedding AWS integration GeminiEmbedding Google's Gemini embedding High performance GLMEmbedding ChatGLM embeddings Chinese language support OllamaEmbedding Local embedding with Ollama Self-hosted option PPIOEmbedding PPIO cloud embedding Scalable solution SiliconflowEmbedding Siliconflow's models Enterprise support VolcengineEmbedding Volcengine embedding High throughput NovitaEmbedding Novita AI embedding Cost-effective"},{"location":"configuration/embedding/#provider-examples","title":"\ud83d\udd0d Provider Examples","text":""},{"location":"configuration/embedding/#openai-embedding","title":"OpenAI Embedding","text":"<p><pre><code>config.set_provider_config(\"embedding\", \"OpenAIEmbedding\", {\"model\": \"text-embedding-3-small\"})\n</code></pre> Requires <code>OPENAI_API_KEY</code> environment variable</p>"},{"location":"configuration/embedding/#milvus-built-in-embedding","title":"Milvus Built-in Embedding","text":"<pre><code>config.set_provider_config(\"embedding\", \"MilvusEmbedding\", {\"model\": \"BAAI/bge-base-en-v1.5\"})\n</code></pre> <p><pre><code>config.set_provider_config(\"embedding\", \"MilvusEmbedding\", {\"model\": \"jina-embeddings-v3\"})\n</code></pre> For Jina's embedding model, requires <code>JINAAI_API_KEY</code> environment variable</p>"},{"location":"configuration/embedding/#voyageai-embedding","title":"VoyageAI Embedding","text":"<p><pre><code>config.set_provider_config(\"embedding\", \"VoyageEmbedding\", {\"model\": \"voyage-3\"})\n</code></pre> Requires <code>VOYAGE_API_KEY</code> environment variable and <code>pip install voyageai</code></p>"},{"location":"configuration/embedding/#additional-providers","title":"\ud83d\udcda Additional Providers","text":"Amazon Bedrock <p><pre><code>config.set_provider_config(\"embedding\", \"BedrockEmbedding\", {\"model\": \"amazon.titan-embed-text-v2:0\"})\n</code></pre> Requires <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> environment variables and <code>pip install boto3</code></p> Novita AI <p><pre><code>config.set_provider_config(\"embedding\", \"NovitaEmbedding\", {\"model\": \"baai/bge-m3\"})\n</code></pre> Requires <code>NOVITA_API_KEY</code> environment variable</p> Siliconflow <p><pre><code>config.set_provider_config(\"embedding\", \"SiliconflowEmbedding\", {\"model\": \"BAAI/bge-m3\"})\n</code></pre> Requires <code>SILICONFLOW_API_KEY</code> environment variable</p> Volcengine <p><pre><code>config.set_provider_config(\"embedding\", \"VolcengineEmbedding\", {\"model\": \"doubao-embedding-text-240515\"})\n</code></pre> Requires <code>VOLCENGINE_API_KEY</code> environment variable</p> GLM <p><pre><code>config.set_provider_config(\"embedding\", \"GLMEmbedding\", {\"model\": \"embedding-3\"})\n</code></pre> Requires <code>GLM_API_KEY</code> environment variable and <code>pip install zhipuai</code></p> Google Gemini <p><pre><code>config.set_provider_config(\"embedding\", \"GeminiEmbedding\", {\"model\": \"text-embedding-004\"})\n</code></pre> Requires <code>GEMINI_API_KEY</code> environment variable and <code>pip install google-genai</code></p> Ollama <p><pre><code>config.set_provider_config(\"embedding\", \"OllamaEmbedding\", {\"model\": \"bge-m3\"})\n</code></pre> Requires local Ollama installation and <code>pip install ollama</code></p> PPIO <p><pre><code>config.set_provider_config(\"embedding\", \"PPIOEmbedding\", {\"model\": \"baai/bge-m3\"})\n</code></pre> Requires <code>PPIO_API_KEY</code> environment variable </p>"},{"location":"configuration/file_loader/","title":"File Loader Configuration","text":"<p>DeepSearcher supports various file loaders to extract and process content from different file formats.</p>"},{"location":"configuration/file_loader/#basic-configuration","title":"\ud83d\udcdd Basic Configuration","text":"<pre><code>config.set_provider_config(\"file_loader\", \"(FileLoaderName)\", \"(Arguments dict)\")\n</code></pre>"},{"location":"configuration/file_loader/#available-file-loaders","title":"\ud83d\udccb Available File Loaders","text":"Loader Description Supported Formats UnstructuredLoader General purpose document loader with broad format support PDF, DOCX, PPT, HTML, etc. DoclingLoader Document processing library with extraction capabilities See documentation"},{"location":"configuration/file_loader/#file-loader-options","title":"\ud83d\udd0d File Loader Options","text":""},{"location":"configuration/file_loader/#unstructured","title":"Unstructured","text":"<p>Unstructured is a powerful library for extracting content from various document formats.</p> <pre><code>config.set_provider_config(\"file_loader\", \"UnstructuredLoader\", {})\n</code></pre> Setup Instructions <p>You can use Unstructured in two ways:</p> <ol> <li>With API (recommended for production)</li> <li> <p>Set environment variables:</p> <ul> <li><code>UNSTRUCTURED_API_KEY</code></li> <li><code>UNSTRUCTURED_API_URL</code></li> </ul> </li> <li> <p>Local Processing</p> </li> <li>Simply don't set the API environment variables</li> <li>Install required dependencies:      <pre><code># Install core dependencies\npip install unstructured-ingest\n\n# For all document formats\npip install \"unstructured[all-docs]\"\n\n# For specific formats (e.g., PDF only)\npip install \"unstructured[pdf]\"\n</code></pre></li> </ol> <p>For more information: - Unstructured Documentation - Installation Guide</p>"},{"location":"configuration/file_loader/#docling","title":"Docling","text":"<p>Docling provides document processing capabilities with support for multiple formats.</p> <pre><code>config.set_provider_config(\"file_loader\", \"DoclingLoader\", {})\n</code></pre> Setup Instructions <ol> <li> <p>Install Docling:    <pre><code>pip install docling\n</code></pre></p> </li> <li> <p>For information on supported formats, see the Docling documentation. </p> </li> </ol>"},{"location":"configuration/llm/","title":"LLM Configuration","text":"<p>DeepSearcher supports various Large Language Models (LLMs) for processing queries and generating responses.</p>"},{"location":"configuration/llm/#basic-configuration","title":"\ud83d\udcdd Basic Configuration","text":"<pre><code>config.set_provider_config(\"llm\", \"(LLMName)\", \"(Arguments dict)\")\n</code></pre>"},{"location":"configuration/llm/#available-llm-providers","title":"\ud83d\udccb Available LLM Providers","text":"Provider Description Key Models OpenAI OpenAI's API for GPT models o1-mini, GPT-4 DeepSeek DeepSeek AI offering deepseek-reasoner, coder Anthropic Anthropic's Claude models claude-3-opus, claude-3-sonnet Gemini Google's Gemini models gemini-1.5-pro, gemini-2.0-flash XAI X.AI's Grok models grok-2-latest Ollama Local LLM deployment llama3, qwq, etc. SiliconFlow Enterprise AI platform deepseek-r1 TogetherAI Multiple model options llama-4, deepseek PPIO Cloud AI infrastructure deepseek, llama Volcengine ByteDance LLM platform deepseek-r1 GLM ChatGLM models glm-4-plus Bedrock Amazon Bedrock LLMs anthropic.claude, ai21.j2 Novita Novita AI models Various options"},{"location":"configuration/llm/#provider-examples","title":"\ud83d\udd0d Provider Examples","text":""},{"location":"configuration/llm/#openai","title":"OpenAI","text":"<p><pre><code>config.set_provider_config(\"llm\", \"OpenAI\", {\"model\": \"o1-mini\"})\n</code></pre> Requires <code>OPENAI_API_KEY</code> environment variable</p>"},{"location":"configuration/llm/#deepseek","title":"DeepSeek","text":"<p><pre><code>config.set_provider_config(\"llm\", \"DeepSeek\", {\"model\": \"deepseek-reasoner\"})\n</code></pre> Requires <code>DEEPSEEK_API_KEY</code> environment variable</p>"},{"location":"configuration/llm/#additional-providers","title":"\ud83d\udcda Additional Providers","text":"DeepSeek from SiliconFlow <p><pre><code>config.set_provider_config(\"llm\", \"SiliconFlow\", {\"model\": \"deepseek-ai/DeepSeek-R1\"})\n</code></pre> Requires <code>SILICONFLOW_API_KEY</code> environment variable</p> <p>More details about SiliconFlow: https://docs.siliconflow.cn/quickstart</p> DeepSeek from TogetherAI <p>Requires <code>TOGETHER_API_KEY</code> environment variable and <code>pip install together</code></p> <p>For DeepSeek R1: <pre><code>config.set_provider_config(\"llm\", \"TogetherAI\", {\"model\": \"deepseek-ai/DeepSeek-R1\"})\n</code></pre></p> <p>For Llama 4: <pre><code>config.set_provider_config(\"llm\", \"TogetherAI\", {\"model\": \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"})\n</code></pre></p> <p>More details about TogetherAI: https://www.together.ai/</p> XAI Grok <p><pre><code>config.set_provider_config(\"llm\", \"XAI\", {\"model\": \"grok-2-latest\"})\n</code></pre> Requires <code>XAI_API_KEY</code> environment variable</p> <p>More details about XAI Grok: https://docs.x.ai/docs/overview#featured-models</p> Claude <p><pre><code>config.set_provider_config(\"llm\", \"Anthropic\", {\"model\": \"claude-3-7-sonnet-latest\"})\n</code></pre> Requires <code>ANTHROPIC_API_KEY</code> environment variable</p> <p>More details about Anthropic Claude: https://docs.anthropic.com/en/home</p> Google Gemini <p><pre><code>config.set_provider_config('llm', 'Gemini', { 'model': 'gemini-2.0-flash' })\n</code></pre> Requires <code>GEMINI_API_KEY</code> environment variable and <code>pip install google-genai</code></p> <p>More details about Gemini: https://ai.google.dev/gemini-api/docs</p> DeepSeek from PPIO <p><pre><code>config.set_provider_config(\"llm\", \"PPIO\", {\"model\": \"deepseek/deepseek-r1-turbo\"})\n</code></pre> Requires <code>PPIO_API_KEY</code> environment variable</p> <p>More details about PPIO: https://ppinfra.com/docs/get-started/quickstart.html</p> Ollama <pre><code>config.set_provider_config(\"llm\", \"Ollama\", {\"model\": \"qwq\"})\n</code></pre> <p>Follow these instructions to set up and run a local Ollama instance:</p> <ol> <li>Download and install Ollama</li> <li>View available models via the model library</li> <li>Pull models with <code>ollama pull &lt;name-of-model&gt;</code></li> <li>By default, Ollama has a REST API on http://localhost:11434</li> </ol> Volcengine <p><pre><code>config.set_provider_config(\"llm\", \"Volcengine\", {\"model\": \"deepseek-r1-250120\"})\n</code></pre> Requires <code>VOLCENGINE_API_KEY</code> environment variable</p> <p>More details about Volcengine: https://www.volcengine.com/docs/82379/1099455</p> GLM <p><pre><code>config.set_provider_config(\"llm\", \"GLM\", {\"model\": \"glm-4-plus\"})\n</code></pre> Requires <code>GLM_API_KEY</code> environment variable and <code>pip install zhipuai</code></p> <p>More details about GLM: https://bigmodel.cn/dev/welcome</p> Amazon Bedrock <p><pre><code>config.set_provider_config(\"llm\", \"Bedrock\", {\"model\": \"us.deepseek.r1-v1:0\"})\n</code></pre> Requires <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> environment variables and <code>pip install boto3</code></p> <p>More details about Amazon Bedrock: https://docs.aws.amazon.com/bedrock/</p> Aliyun Bailian <p><pre><code>config.set_provider_config(\"llm\", \"OpenAI\", {\"model\": \"deepseek-r1\", \"base_url\": \"https://dashscope.aliyuncs.com/compatible-mode/v1\"})\n</code></pre> Requires <code>OPENAI_API_KEY</code> environment variable</p> <p>More details about Aliyun Bailian models: https://bailian.console.aliyun.com </p>"},{"location":"configuration/vector_db/","title":"Vector Database Configuration","text":"<p>DeepSearcher uses vector databases to store and retrieve document embeddings for efficient semantic search.</p>"},{"location":"configuration/vector_db/#basic-configuration","title":"\ud83d\udcdd Basic Configuration","text":"<pre><code>config.set_provider_config(\"vector_db\", \"(VectorDBName)\", \"(Arguments dict)\")\n</code></pre> <p>Currently supported vector databases: - Milvus (including Milvus Lite and Zilliz Cloud)</p>"},{"location":"configuration/vector_db/#milvus-configuration","title":"\ud83d\udd0d Milvus Configuration","text":"<pre><code>config.set_provider_config(\"vector_db\", \"Milvus\", {\"uri\": \"./milvus.db\", \"token\": \"\"})\n</code></pre>"},{"location":"configuration/vector_db/#deployment-options","title":"Deployment Options","text":"Local Storage with Milvus Lite <p>Setting the <code>uri</code> as a local file (e.g., <code>./milvus.db</code>) automatically utilizes Milvus Lite to store all data in this file. This is the most convenient method for development and smaller datasets.</p> <pre><code>config.set_provider_config(\"vector_db\", \"Milvus\", {\"uri\": \"./milvus.db\", \"token\": \"\"})\n</code></pre> Standalone Milvus Server <p>For larger datasets, you can set up a more performant Milvus server using Docker or Kubernetes. In this setup, use the server URI as your <code>uri</code> parameter:</p> <pre><code>config.set_provider_config(\"vector_db\", \"Milvus\", {\"uri\": \"http://localhost:19530\", \"token\": \"\"})\n</code></pre> Zilliz Cloud (Managed Service) <p>Zilliz Cloud provides a fully managed cloud service for Milvus. To use Zilliz Cloud, adjust the <code>uri</code> and <code>token</code> according to the Public Endpoint and API Key:</p> <pre><code>config.set_provider_config(\"vector_db\", \"Milvus\", {\n    \"uri\": \"https://your-instance-id.api.gcp-us-west1.zillizcloud.com\", \n    \"token\": \"your_api_key\"\n})\n</code></pre>"},{"location":"configuration/web_crawler/","title":"Web Crawler Configuration","text":"<p>DeepSearcher supports various web crawlers to collect data from websites for processing and indexing.</p>"},{"location":"configuration/web_crawler/#basic-configuration","title":"\ud83d\udcdd Basic Configuration","text":"<pre><code>config.set_provider_config(\"web_crawler\", \"(WebCrawlerName)\", \"(Arguments dict)\")\n</code></pre>"},{"location":"configuration/web_crawler/#available-web-crawlers","title":"\ud83d\udccb Available Web Crawlers","text":"Crawler Description Key Feature FireCrawlCrawler Cloud-based web crawling service Simple API, managed service Crawl4AICrawler Browser automation crawler Full JavaScript support JinaCrawler Content extraction service High accuracy parsing DoclingCrawler Doc processing with crawling Multiple format support"},{"location":"configuration/web_crawler/#web-crawler-options","title":"\ud83d\udd0d Web Crawler Options","text":""},{"location":"configuration/web_crawler/#firecrawl","title":"FireCrawl","text":"<p>FireCrawl is a cloud-based web crawling service designed for AI applications.</p> <p>Key features: - Simple API - Managed Service - Advanced Parsing</p> <pre><code>config.set_provider_config(\"web_crawler\", \"FireCrawlCrawler\", {})\n</code></pre> Setup Instructions <ol> <li>Sign up for FireCrawl and get an API key</li> <li>Set the API key as an environment variable:    <pre><code>export FIRECRAWL_API_KEY=\"your_api_key\"\n</code></pre></li> <li>For more information, see the FireCrawl documentation</li> </ol>"},{"location":"configuration/web_crawler/#crawl4ai","title":"Crawl4AI","text":"<p>Crawl4AI is a Python package for web crawling with browser automation capabilities.</p> <pre><code>config.set_provider_config(\"web_crawler\", \"Crawl4AICrawler\", {\"browser_config\": {\"headless\": True, \"verbose\": True}})\n</code></pre> Setup Instructions <ol> <li>Install Crawl4AI:    <pre><code>pip install crawl4ai\n</code></pre></li> <li>Run the setup command:    <pre><code>crawl4ai-setup\n</code></pre></li> <li>For more information, see the Crawl4AI documentation</li> </ol>"},{"location":"configuration/web_crawler/#jina-reader","title":"Jina Reader","text":"<p>Jina Reader is a service for extracting content from web pages with high accuracy.</p> <pre><code>config.set_provider_config(\"web_crawler\", \"JinaCrawler\", {})\n</code></pre> Setup Instructions <ol> <li>Get a Jina API key</li> <li>Set the API key as an environment variable:    <pre><code>export JINA_API_TOKEN=\"your_api_key\"\n# or\nexport JINAAI_API_KEY=\"your_api_key\"\n</code></pre></li> <li>For more information, see the Jina Reader documentation</li> </ol>"},{"location":"configuration/web_crawler/#docling-crawler","title":"Docling Crawler","text":"<p>Docling provides web crawling capabilities alongside its document processing features.</p> <pre><code>config.set_provider_config(\"web_crawler\", \"DoclingCrawler\", {})\n</code></pre> Setup Instructions <ol> <li>Install Docling:    <pre><code>pip install docling\n</code></pre></li> <li>For information on supported formats, see the Docling documentation </li> </ol>"},{"location":"examples/","title":"Usage Examples","text":"<p>DeepSearcher provides several example scripts to help you get started quickly. These examples demonstrate different ways to use DeepSearcher for various use cases.</p>"},{"location":"examples/#available-examples","title":"\ud83d\udccb Available Examples","text":"Example Description Key Features Basic Example Simple example showing core functionality Loading PDFs, querying Docling Integration Using Docling for file loading and web crawling Multiple sources, local and web Unstructured Integration Using Unstructured for parsing documents API and local processing FireCrawl Integration Web crawling with FireCrawl Website data extraction Oracle Setup Advanced configuration with Oracle Path setup, token tracking <p>Click on any example to see detailed code and explanations. </p>"},{"location":"examples/basic_example/","title":"Basic Example2","text":"<p>This example demonstrates the core functionality of DeepSearcher - loading documents and performing semantic search.</p>"},{"location":"examples/basic_example/#overview","title":"Overview","text":"<p>The script performs these steps:</p> <ol> <li>Configures DeepSearcher with default settings</li> <li>Loads a PDF document about Milvus</li> <li>Asks a question about Milvus and vector databases</li> <li>Displays token usage information</li> </ol>"},{"location":"examples/basic_example/#code-example","title":"Code Example","text":"<pre><code>import logging\nimport os\n\nfrom deepsearcher.offline_loading import load_from_local_files\nfrom deepsearcher.online_query import query\nfrom deepsearcher.configuration import Configuration, init_config\n\nhttpx_logger = logging.getLogger(\"httpx\")  # disable openai's logger output\nhttpx_logger.setLevel(logging.WARNING)\n\ncurrent_dir = os.path.dirname(os.path.abspath(__file__))\n\nconfig = Configuration()  # Customize your config here\ninit_config(config=config)\n\n\n# You should clone the milvus docs repo to your local machine first, execute:\n# git clone https://github.com/milvus-io/milvus-docs.git\n# Then replace the path below with the path to the milvus-docs repo on your local machine\n# import glob\n# all_md_files = glob.glob('xxx/milvus-docs/site/en/**/*.md', recursive=True)\n# load_from_local_files(paths_or_directory=all_md_files, collection_name=\"milvus_docs\", collection_description=\"All Milvus Documents\")\n\n# Hint: You can also load a single file, please execute it in the root directory of the deep searcher project\nload_from_local_files(\n    paths_or_directory=os.path.join(current_dir, \"data/WhatisMilvus.pdf\"),\n    collection_name=\"milvus_docs\",\n    collection_description=\"All Milvus Documents\",\n    # force_new_collection=True, # If you want to drop origin collection and create a new collection every time, set force_new_collection to True\n)\n\nquestion = \"Write a report comparing Milvus with other vector databases.\"\n\n_, _, consumed_token = query(question, max_iter=1)\nprint(f\"Consumed tokens: {consumed_token}\")\n</code></pre>"},{"location":"examples/basic_example/#running-the-example","title":"Running the Example","text":"<ol> <li>Make sure you have installed DeepSearcher: <code>pip install deepsearcher</code></li> <li>Create a data directory and add a PDF about Milvus (or use your own data)</li> <li>Run the script: <code>python basic_example.py</code></li> </ol>"},{"location":"examples/basic_example/#key-concepts","title":"Key Concepts","text":"<ul> <li>Configuration: Using the default configuration</li> <li>Document Loading: Loading a single PDF file</li> <li>Querying: Asking a complex question requiring synthesis of information</li> <li>Token Tracking: Monitoring token usage from the LLM </li> </ul>"},{"location":"examples/docling/","title":"Docling Integration Example","text":"<p>This example shows how to use Docling for loading local files and crawling web content.</p>"},{"location":"examples/docling/#overview","title":"Overview","text":"<p>The script demonstrates:</p> <ol> <li>Configuring DeepSearcher to use Docling for both file loading and web crawling</li> <li>Loading data from local files using Docling's document parser</li> <li>Crawling web content from multiple sources including Markdown and PDF files</li> <li>Querying the loaded data</li> </ol>"},{"location":"examples/docling/#code-example","title":"Code Example","text":"<pre><code>import logging\nimport os\nfrom deepsearcher.offline_loading import load_from_local_files, load_from_website\nfrom deepsearcher.online_query import query\nfrom deepsearcher.configuration import Configuration, init_config\n\n# Suppress unnecessary logging from third-party libraries\nlogging.getLogger(\"httpx\").setLevel(logging.WARNING)\n\ndef main():\n    # Step 1: Initialize configuration\n    config = Configuration()\n\n    # Configure Vector Database and Docling providers\n    config.set_provider_config(\"vector_db\", \"Milvus\", {})\n    config.set_provider_config(\"file_loader\", \"DoclingLoader\", {})\n    config.set_provider_config(\"web_crawler\", \"DoclingCrawler\", {})\n\n    # Apply the configuration\n    init_config(config)\n\n    # Step 2a: Load data from a local file using DoclingLoader\n    local_file = \"your_local_file_or_directory\"\n    local_collection_name = \"DoclingLocalFiles\"\n    local_collection_description = \"Milvus Documents loaded using DoclingLoader\"\n\n    print(\"\\n=== Loading local files using DoclingLoader ===\")\n\n    try:\n        load_from_local_files(\n            paths_or_directory=local_file, \n            collection_name=local_collection_name, \n            collection_description=local_collection_description,\n            force_new_collection=True\n        )\n        print(f\"Successfully loaded: {local_file}\")\n    except ValueError as e:\n        print(f\"Validation error: {str(e)}\")\n    except Exception as e:\n        print(f\"Error: {str(e)}\")\n\n    print(\"Successfully loaded all local files\")\n\n    # Step 2b: Crawl URLs using DoclingCrawler\n    urls = [\n        # Markdown documentation files\n        \"https://milvus.io/docs/quickstart.md\",\n        \"https://milvus.io/docs/overview.md\",\n        # PDF example - can handle various URL formats\n        \"https://arxiv.org/pdf/2408.09869\",\n    ]\n    web_collection_name = \"DoclingWebCrawl\"\n    web_collection_description = \"Milvus Documentation crawled using DoclingCrawler\"\n\n    print(\"\\n=== Crawling web pages using DoclingCrawler ===\")\n\n    load_from_website(\n        urls=urls,\n        collection_name=web_collection_name,\n        collection_description=web_collection_description,\n        force_new_collection=True\n    )\n    print(\"Successfully crawled all URLs\")\n\n    # Step 3: Query the loaded data\n    question = \"What is Milvus?\"\n    result = query(question)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/docling/#running-the-example","title":"Running the Example","text":"<ol> <li>Install DeepSearcher and Docling: <code>pip install deepsearcher docling</code></li> <li>Replace <code>your_local_file_or_directory</code> with your actual file/directory path</li> <li>Run the script: <code>python load_and_crawl_using_docling.py</code></li> </ol>"},{"location":"examples/docling/#key-concepts","title":"Key Concepts","text":"<ul> <li>Multiple Providers: Configuring both file loader and web crawler to use Docling</li> <li>Local Files: Loading documents from your local filesystem</li> <li>Web Crawling: Retrieving content from multiple web URLs with different formats</li> <li>Error Handling: Graceful error handling for loading operations </li> </ul>"},{"location":"examples/firecrawl/","title":"FireCrawl Integration Example","text":"<p>This example demonstrates how to use FireCrawl with DeepSearcher to crawl and extract content from websites.</p>"},{"location":"examples/firecrawl/#overview","title":"Overview","text":"<p>FireCrawl is a specialized web crawling service designed for AI applications. This example shows:</p> <ol> <li>Setting up FireCrawl with DeepSearcher</li> <li>Configuring API keys for the service</li> <li>Crawling a website and extracting content</li> <li>Querying the extracted content</li> </ol>"},{"location":"examples/firecrawl/#code-example","title":"Code Example","text":"<pre><code>import logging\nimport os\nfrom deepsearcher.offline_loading import load_from_website\nfrom deepsearcher.online_query import query\nfrom deepsearcher.configuration import Configuration, init_config\n\n# Suppress unnecessary logging from third-party libraries\nlogging.getLogger(\"httpx\").setLevel(logging.WARNING)\n\n# Set API keys (ensure these are set securely in real applications)\nos.environ['OPENAI_API_KEY'] = 'sk-***************'\nos.environ['FIRECRAWL_API_KEY'] = 'fc-***************'\n\n\ndef main():\n    # Step 1: Initialize configuration\n    config = Configuration()\n\n    # Set up Vector Database (Milvus) and Web Crawler (FireCrawlCrawler)\n    config.set_provider_config(\"vector_db\", \"Milvus\", {})\n    config.set_provider_config(\"web_crawler\", \"FireCrawlCrawler\", {})\n\n    # Apply the configuration\n    init_config(config)\n\n    # Step 2: Load data from a website into Milvus\n    website_url = \"https://example.com\"  # Replace with your target website\n    collection_name = \"FireCrawl\"\n    collection_description = \"All Milvus Documents\"\n\n    # crawl a single webpage\n    load_from_website(urls=website_url, collection_name=collection_name, collection_description=collection_description)\n    # only applicable if using Firecrawl: deepsearcher can crawl multiple webpages, by setting max_depth, limit, allow_backward_links\n    # load_from_website(urls=website_url, max_depth=2, limit=20, allow_backward_links=True, collection_name=collection_name, collection_description=collection_description)\n\n    # Step 3: Query the loaded data\n    question = \"What is Milvus?\"  # Replace with your actual question\n    result = query(question)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/firecrawl/#running-the-example","title":"Running the Example","text":"<ol> <li>Install DeepSearcher: <code>pip install deepsearcher</code></li> <li>Sign up for a FireCrawl API key at firecrawl.dev</li> <li>Replace the placeholder API keys with your actual keys</li> <li>Change the <code>website_url</code> to the website you want to crawl</li> <li>Run the script: <code>python load_website_using_firecrawl.py</code></li> </ol>"},{"location":"examples/firecrawl/#advanced-crawling-options","title":"Advanced Crawling Options","text":"<p>FireCrawl provides several advanced options for crawling:</p> <ul> <li><code>max_depth</code>: Control how many links deep the crawler should go</li> <li><code>limit</code>: Set a maximum number of pages to crawl</li> <li><code>allow_backward_links</code>: Allow the crawler to navigate to parent/sibling pages</li> </ul>"},{"location":"examples/firecrawl/#key-concepts","title":"Key Concepts","text":"<ul> <li>Web Crawling: Extracting content from websites</li> <li>Depth Control: Managing how deep the crawler navigates</li> <li>URL Processing: Handling multiple pages from a single starting point</li> <li>Vector Storage: Storing the crawled content in a vector database for search </li> </ul>"},{"location":"examples/oracle/","title":"Oracle Example","text":"<p>This example demonstrates an advanced setup using path manipulation and detailed token tracking.</p>"},{"location":"examples/oracle/#overview","title":"Overview","text":"<p>This example shows:</p> <ol> <li>Setting up Python path for importing from the parent directory</li> <li>Initializing DeepSearcher with default configuration</li> <li>Loading a PDF document and creating a vector database</li> <li>Performing a complex query with full result and token tracking</li> <li>Optional token consumption monitoring</li> </ol>"},{"location":"examples/oracle/#code-example","title":"Code Example","text":"<pre><code>import sys, os\nfrom pathlib import Path\nscript_directory = Path(__file__).resolve().parent.parent\nsys.path.append(os.path.abspath(script_directory))\n\nimport logging\n\nhttpx_logger = logging.getLogger(\"httpx\")  # disable openai's logger output\nhttpx_logger.setLevel(logging.WARNING)\n\ncurrent_dir = os.path.dirname(os.path.abspath(__file__))\n\n# Customize your config here\nfrom deepsearcher.configuration import Configuration, init_config\n\nconfig = Configuration()\ninit_config(config=config)\n\n# Load your local data\n# Hint: You can load from a directory or a single file, please execute it in the root directory of the deep searcher project\n\nfrom deepsearcher.offline_loading import load_from_local_files\n\nload_from_local_files(\n    paths_or_directory=os.path.join(current_dir, \"data/WhatisMilvus.pdf\"),\n    collection_name=\"milvus_docs\",\n    collection_description=\"All Milvus Documents\",\n    # force_new_collection=True, # If you want to drop origin collection and create a new collection every time, set force_new_collection to True\n)\n\n# Query\nfrom deepsearcher.online_query import query\n\nquestion = 'Write a report comparing Milvus with other vector databases.'\nanswer, retrieved_results, consumed_token = query(question)\nprint(answer)\n\n# get consumed tokens, about: 2.5~3w tokens when using openai gpt-4o model\n# print(f\"Consumed tokens: {consumed_token}\")\n</code></pre>"},{"location":"examples/oracle/#running-the-example","title":"Running the Example","text":"<ol> <li>Install DeepSearcher: <code>pip install deepsearcher</code></li> <li>Make sure you have the data directory with \"WhatisMilvus.pdf\" (or change the path)</li> <li>Run the script: <code>python basic_example_oracle.py</code></li> </ol>"},{"location":"examples/oracle/#key-concepts","title":"Key Concepts","text":"<ul> <li>Path Management: Setting up Python path to import from parent directory</li> <li>Query Unpacking: Getting full result details (answer, retrieved context, and tokens)</li> <li>Complex Querying: Asking for a comparative analysis that requires synthesis</li> <li>Token Economy: Monitoring token usage for cost optimization </li> </ul>"},{"location":"examples/unstructured/","title":"Unstructured Integration Example","text":"<p>This example demonstrates how to use the Unstructured library with DeepSearcher for advanced document parsing.</p>"},{"location":"examples/unstructured/#overview","title":"Overview","text":"<p>Unstructured is a powerful document processing library that can extract content from various document formats. This example shows:</p> <ol> <li>Setting up Unstructured with DeepSearcher</li> <li>Configuring the Unstructured API keys (optional)</li> <li>Loading documents with Unstructured's parser</li> <li>Querying the extracted content</li> </ol>"},{"location":"examples/unstructured/#code-example","title":"Code Example","text":"<pre><code>import logging\nimport os\nfrom deepsearcher.offline_loading import load_from_local_files\nfrom deepsearcher.online_query import query\nfrom deepsearcher.configuration import Configuration, init_config\n\n# Suppress unnecessary logging from third-party libraries\nlogging.getLogger(\"httpx\").setLevel(logging.WARNING)\n\n# (Optional) Set API keys (ensure these are set securely in real applications)\nos.environ['UNSTRUCTURED_API_KEY'] = '***************'\nos.environ['UNSTRUCTURED_API_URL'] = '***************'\n\n\ndef main():\n    # Step 1: Initialize configuration\n    config = Configuration()\n\n    # Configure Vector Database (Milvus) and File Loader (UnstructuredLoader)\n    config.set_provider_config(\"vector_db\", \"Milvus\", {})\n    config.set_provider_config(\"file_loader\", \"UnstructuredLoader\", {})\n\n    # Apply the configuration\n    init_config(config)\n\n    # Step 2: Load data from a local file or directory into Milvus\n    input_file = \"your_local_file_or_directory\"  # Replace with your actual file path\n    collection_name = \"Unstructured\"\n    collection_description = \"All Milvus Documents\"\n\n    load_from_local_files(paths_or_directory=input_file, collection_name=collection_name, collection_description=collection_description)\n\n    # Step 3: Query the loaded data\n    question = \"What is Milvus?\"  # Replace with your actual question\n    result = query(question)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/unstructured/#running-the-example","title":"Running the Example","text":"<ol> <li>Install DeepSearcher with Unstructured support: <code>pip install deepsearcher \"unstructured[all-docs]\"</code></li> <li>(Optional) Sign up for the Unstructured API at unstructured.io if you want to use their cloud service</li> <li>Replace <code>your_local_file_or_directory</code> with your own document file path or directory</li> <li>Run the script: <code>python load_local_file_using_unstructured.py</code></li> </ol>"},{"location":"examples/unstructured/#unstructured-options","title":"Unstructured Options","text":"<p>You can use Unstructured in two modes:</p> <ol> <li>API Mode: Set the environment variables <code>UNSTRUCTURED_API_KEY</code> and <code>UNSTRUCTURED_API_URL</code> to use their cloud service</li> <li>Local Mode: Don't set the environment variables, and Unstructured will process documents locally on your machine</li> </ol>"},{"location":"examples/unstructured/#key-concepts","title":"Key Concepts","text":"<ul> <li>Document Processing: Advanced document parsing for various formats</li> <li>API/Local Options: Flexibility in deployment based on your needs</li> <li>Integration: Seamless integration with DeepSearcher's vector database and query capabilities </li> </ul>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#common-issues-and-solutions","title":"\ud83d\udd0d Common Issues and Solutions","text":""},{"location":"faq/#q1-why-am-i-failing-to-parse-llm-output-format-how-to-select-the-right-llm","title":"\ud83d\udcac Q1: Why am I failing to parse LLM output format / How to select the right LLM?","text":"<p>Solution: Small language models often struggle to follow prompts and generate responses in the expected format. For better results, we recommend using large reasoning models such as:</p> <ul> <li>DeepSeek-R1 671B</li> <li>OpenAI o-series models</li> <li>Claude 3.7 Sonnet</li> </ul> <p>These models provide superior reasoning capabilities and are more likely to produce correctly formatted outputs.</p>"},{"location":"faq/#q2-we-couldnt-connect-to-httpshuggingfaceco-error","title":"\ud83c\udf10 Q2: \"We couldn't connect to 'https://huggingface.co'\" error","text":"<p>Error Message:</p>  OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like GPTCache/paraphrase-albert-small-v2 is not the path to a directory containing a file named config.json. Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.  <p>Solution: This issue is typically caused by network access problems to Hugging Face. Try these solutions:</p> Network Issue? Try Using a Mirror <pre><code>export HF_ENDPOINT=https://hf-mirror.com\n</code></pre> Permission Issue? Set Up a Personal Token <pre><code>export HUGGING_FACE_HUB_TOKEN=xxxx\n</code></pre>"},{"location":"faq/#q3-deepsearcher-doesnt-run-in-jupyter-notebook","title":"\ud83d\udcd3 Q3: DeepSearcher doesn't run in Jupyter notebook","text":"<p>Solution: This is a common issue with asyncio in Jupyter notebooks. Install <code>nest_asyncio</code> and add the following code to the top of your notebook:</p> <p>Step 1: Install the required package</p> <pre><code>pip install nest_asyncio\n</code></pre> <p>Step 2: Add these lines to the beginning of your notebook</p> <pre><code>import nest_asyncio\nnest_asyncio.apply()\n</code></pre> <p> </p>"},{"location":"installation/","title":"\ud83d\udd27 Installation","text":"<p>DeepSearcher offers multiple installation methods to suit different user needs.</p>"},{"location":"installation/#installation-options","title":"\ud83d\udccb Installation Options","text":"Method Best For Description \ud83d\udce6 Installation via pip Most users Quick and easy installation using pip package manager \ud83d\udee0\ufe0f Development mode Contributors Setup for those who want to modify the code or contribute"},{"location":"installation/#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>Once installed, you can verify your installation:</p> <pre><code>from deepsearcher.configuration import Configuration\nfrom deepsearcher.online_query import query\n\n# Initialize with default configuration\nconfig = Configuration()\nprint(\"DeepSearcher installed successfully!\")\n</code></pre>"},{"location":"installation/#system-requirements","title":"\ud83d\udcbb System Requirements","text":"<ul> <li>Python 3.10 or higher</li> <li>4GB RAM minimum (8GB+ recommended)</li> <li>Internet connection for downloading models and dependencies </li> </ul>"},{"location":"installation/development/","title":"\ud83d\udee0\ufe0f Development Mode Installation","text":"<p>This guide is for contributors who want to modify DeepSearcher's code or develop new features.</p>"},{"location":"installation/development/#prerequisites","title":"\ud83d\udccb Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>git</li> <li>uv package manager (recommended for faster installation)</li> </ul>"},{"location":"installation/development/#installation-steps","title":"\ud83d\udd04 Installation Steps","text":""},{"location":"installation/development/#step-1-install-uv-recommended","title":"Step 1: Install uv (Recommended)","text":"<p>uv is a faster alternative to pip for Python package management.</p> Using pipUsing curl (Unix/macOS)Using PowerShell (Windows) <pre><code>pip install uv\n</code></pre> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <pre><code>irm https://astral.sh/uv/install.ps1 | iex\n</code></pre> <p>For more options, see the official uv installation guide.</p>"},{"location":"installation/development/#step-2-clone-the-repository","title":"Step 2: Clone the repository","text":"<pre><code>git clone https://github.com/zilliztech/deep-searcher.git\ncd deep-searcher\n</code></pre>"},{"location":"installation/development/#step-3-set-up-the-development-environment","title":"Step 3: Set up the development environment","text":"Using uv (Recommended)Using pip <pre><code>uv sync\nsource .venv/bin/activate\n</code></pre> <pre><code>python -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\npip install -e \".[dev,all]\"\n</code></pre>"},{"location":"installation/development/#running-tests","title":"\ud83e\uddea Running Tests","text":"<pre><code>pytest tests/\n</code></pre>"},{"location":"installation/development/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<p>For more detailed development setup instructions, including contribution guidelines, code style, and testing procedures, please refer to the CONTRIBUTING.md file in the repository. </p>"},{"location":"installation/pip/","title":"\ud83d\udce6 Installation via pip","text":"<p>This method is recommended for most users who want to use DeepSearcher without modifying its source code.</p>"},{"location":"installation/pip/#prerequisites","title":"\ud83d\udccb Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>pip package manager (included with Python)</li> <li>Virtual environment tool (recommended)</li> </ul>"},{"location":"installation/pip/#step-by-step-installation","title":"\ud83d\udd04 Step-by-Step Installation","text":""},{"location":"installation/pip/#step-1-create-a-virtual-environment","title":"Step 1: Create a virtual environment","text":"<pre><code>python -m venv .venv\n</code></pre>"},{"location":"installation/pip/#step-2-activate-the-virtual-environment","title":"Step 2: Activate the virtual environment","text":"Linux/macOSWindows <pre><code>source .venv/bin/activate\n</code></pre> <pre><code>.venv\\Scripts\\activate\n</code></pre>"},{"location":"installation/pip/#step-3-install-deepsearcher","title":"Step 3: Install DeepSearcher","text":"<pre><code>pip install deepsearcher\n</code></pre>"},{"location":"installation/pip/#optional-dependencies","title":"\ud83e\udde9 Optional Dependencies","text":"<p>DeepSearcher supports various integrations through optional dependencies.</p> Integration Command Description Ollama <code>pip install \"deepsearcher[ollama]\"</code> For local LLM deployment All extras <code>pip install \"deepsearcher[all]\"</code> Installs all optional dependencies"},{"location":"installation/pip/#verify-installation","title":"\u2705 Verify Installation","text":"<pre><code># Simple verification\nfrom deepsearcher import __version__\nprint(f\"DeepSearcher version: {__version__}\")\n</code></pre>"},{"location":"integrations/","title":"Module Support","text":"<p>DeepSearcher supports various integration modules including embedding models, large language models, document loaders and vector databases.</p>"},{"location":"integrations/#overview","title":"\ud83d\udcca Overview","text":"Module Type Count Description Embedding Models 7+ Text vectorization tools Large Language Models 11+ Query processing and text generation Document Loaders 5+ Parse and process documents in various formats Vector Databases 2+ Store and retrieve vector data"},{"location":"integrations/#embedding-models","title":"\ud83d\udd22 Embedding Models","text":"<p>Support for various embedding models to convert text into vector representations for semantic search.</p> Provider Required Environment Variables Features Open-source models None Locally runnable open-source models OpenAI <code>OPENAI_API_KEY</code> High-quality embeddings, easy to use VoyageAI <code>VOYAGE_API_KEY</code> Embeddings optimized for retrieval Amazon Bedrock <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code> AWS integration, enterprise-grade FastEmbed None Fast lightweight embeddings PPIO <code>PPIO_API_KEY</code> Flexible cloud embeddings Novita AI <code>NOVITA_API_KEY</code> Rich model selection"},{"location":"integrations/#llm-support","title":"\ud83e\udde0 Large Language Models","text":"<p>Support for various large language models (LLMs) to process queries and generate responses.</p> Provider Required Environment Variables Features OpenAI <code>OPENAI_API_KEY</code> GPT model family DeepSeek <code>DEEPSEEK_API_KEY</code> Powerful reasoning capabilities XAI Grok <code>XAI_API_KEY</code> Real-time knowledge and humor Anthropic Claude <code>ANTHROPIC_API_KEY</code> Excellent long-context understanding SiliconFlow <code>SILICONFLOW_API_KEY</code> Enterprise inference service PPIO <code>PPIO_API_KEY</code> Diverse model support TogetherAI <code>TOGETHER_API_KEY</code> Wide range of open-source models Google Gemini <code>GEMINI_API_KEY</code> Google's multimodal models SambaNova <code>SAMBANOVA_API_KEY</code> High-performance AI platform Ollama None Local LLM deployment Novita AI <code>NOVITA_API_KEY</code> Diverse AI services"},{"location":"integrations/#document-loader","title":"\ud83d\udcc4 Document Loader","text":"<p>Support for loading and processing documents from various sources.</p>"},{"location":"integrations/#local-file-loaders","title":"Local File Loaders","text":"Loader Supported Formats Required Environment Variables Built-in Loader PDF, TXT, MD None Unstructured Multiple document formats <code>UNSTRUCTURED_API_KEY</code>, <code>UNSTRUCTURED_URL</code> (optional)"},{"location":"integrations/#web-crawlers","title":"Web Crawlers","text":"Crawler Description Required Environment Variables/Setup FireCrawl Crawler designed for AI applications <code>FIRECRAWL_API_KEY</code> Jina Reader High-accuracy web content extraction <code>JINA_API_TOKEN</code> Crawl4AI Browser automation crawler Run <code>crawl4ai-setup</code> for first-time use"},{"location":"integrations/#vector-database-support","title":"\ud83d\udcbe Vector Database Support","text":"<p>Support for various vector databases for efficient storage and retrieval of embeddings.</p> Database Description Features Milvus Open-source vector database High-performance, scalable Zilliz Cloud Managed Milvus service Fully managed, maintenance-free Qdrant Vector similarity search engine Simple, efficient"},{"location":"usage/","title":"\ud83d\udcda Usage Guide","text":"<p>DeepSearcher provides multiple ways to use the system, including Python API, command line interface, and web service deployment.</p>"},{"location":"usage/#usage-overview","title":"\ud83d\udd0d Usage Overview","text":"Guide Description \ud83d\ude80 Quick Start Quick start guide for Python API integration \ud83d\udcbb Command Line Interface Instructions for using the command line interface \ud83c\udf10 Deployment Guide for deploying as a web service <p>Choose the method that best suits your needs and follow the instructions on the corresponding page.</p>"},{"location":"usage/cli/","title":"\ud83d\udcbb Command Line Interface","text":"<p>DeepSearcher provides a convenient command line interface for loading data and querying.</p>"},{"location":"usage/cli/#loading-data","title":"\ud83d\udce5 Loading Data","text":"<p>Load data from files or URLs:</p> <pre><code>deepsearcher load \"your_local_path_or_url\"\n</code></pre> <p>Load into a specific collection:</p> <pre><code>deepsearcher load \"your_local_path_or_url\" --collection_name \"your_collection_name\" --collection_desc \"your_collection_description\"\n</code></pre>"},{"location":"usage/cli/#examples","title":"Examples","text":""},{"location":"usage/cli/#loading-from-local-files","title":"Loading from local files:","text":"<pre><code># Load a single file\ndeepsearcher load \"/path/to/your/local/file.pdf\"\n\n# Load multiple files at once\ndeepsearcher load \"/path/to/your/local/file1.pdf\" \"/path/to/your/local/file2.md\"\n</code></pre>"},{"location":"usage/cli/#loading-from-url","title":"Loading from URL:","text":"<p>Note: Set <code>FIRECRAWL_API_KEY</code> in your environment variables. See FireCrawl documentation for more details.</p> <pre><code>deepsearcher load \"https://www.wikiwand.com/en/articles/DeepSeek\"\n</code></pre>"},{"location":"usage/cli/#querying-data","title":"\ud83d\udd0d Querying Data","text":"<p>Query your loaded data:</p> <pre><code>deepsearcher query \"Write a report about xxx.\"\n</code></pre>"},{"location":"usage/cli/#help-commands","title":"\u2753 Help Commands","text":"<p>Get general help information:</p> <pre><code>deepsearcher --help\n</code></pre> <p>Get help for specific subcommands:</p> <pre><code># Help for load command\ndeepsearcher load --help\n\n# Help for query command\ndeepsearcher query --help\n</code></pre>"},{"location":"usage/deployment/","title":"\ud83c\udf10 Deployment","text":"<p>This guide explains how to deploy DeepSearcher as a web service.</p>"},{"location":"usage/deployment/#configure-modules","title":"\u2699\ufe0f Configure Modules","text":"<p>You can configure all arguments by modifying the configuration file:</p> <pre><code># config.yaml - https://github.com/zilliztech/deep-searcher/blob/main/config.yaml\nllm:\n  provider: \"OpenAI\"\n  api_key: \"your_openai_api_key_here\"\n  # Additional configuration options...\n</code></pre> <p>Important: Set your <code>OPENAI_API_KEY</code> in the <code>llm</code> section of the YAML file.</p>"},{"location":"usage/deployment/#start-service","title":"\ud83d\ude80 Start Service","text":"<p>The main script will run a FastAPI service with default address <code>localhost:8000</code>:</p> <pre><code>$ python main.py\n</code></pre> <p>Once started, you should see output indicating the service is running successfully.</p>"},{"location":"usage/deployment/#access-via-browser","title":"\ud83d\udd0d Access via Browser","text":"<p>You can access the web service through your browser:</p> <ol> <li>Open your browser and navigate to http://localhost:8000/docs</li> <li>The Swagger UI will display all available API endpoints</li> <li>Click the \"Try it out\" button on any endpoint to interact with it</li> <li>Fill in the required parameters and execute the request</li> </ol> <p>This interactive documentation makes it easy to test and use all DeepSearcher API functionality. </p>"},{"location":"usage/quick_start/","title":"\ud83d\ude80 Quick Start","text":""},{"location":"usage/quick_start/#prerequisites","title":"Prerequisites","text":"<p>\u2705 Before you begin, prepare your <code>OPENAI_API_KEY</code> in your environment variables. If you change the LLM in the configuration, make sure to prepare the corresponding API key.</p>"},{"location":"usage/quick_start/#basic-usage","title":"Basic Usage","text":"<pre><code># Import configuration modules\nfrom deepsearcher.configuration import Configuration, init_config\nfrom deepsearcher.online_query import query\n\n# Initialize configuration\nconfig = Configuration()\n\n# Customize your config here\n# (See the Configuration Details section below for more options)\nconfig.set_provider_config(\"llm\", \"OpenAI\", {\"model\": \"o1-mini\"})\nconfig.set_provider_config(\"embedding\", \"OpenAIEmbedding\", {\"model\": \"text-embedding-ada-002\"})\ninit_config(config=config)\n\n# Load data from local files\nfrom deepsearcher.offline_loading import load_from_local_files\nload_from_local_files(paths_or_directory=your_local_path)\n\n# (Optional) Load data from websites\n# Requires FIRECRAWL_API_KEY environment variable\nfrom deepsearcher.offline_loading import load_from_website\nload_from_website(urls=website_url)\n\n# Query your data\nresult = query(\"Write a report about xxx.\")  # Replace with your question\nprint(result)\n</code></pre>"},{"location":"usage/quick_start/#next-steps","title":"Next Steps","text":"<p>After completing this quick start, you might want to explore:</p> <ul> <li>Command Line Interface for non-programmatic usage</li> <li>Deployment for setting up a web service </li> </ul>"}]}